{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import binascii\n",
    "import numpy as np\n",
    "from pandas import read_csv, DataFrame\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from collections import Iterable, OrderedDict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import stats\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import NE_candidate_module as ne\n",
    "import NE_candidate_module as ne\n",
    "import Mention\n",
    "import threading, queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------Existing Lists--------------------\n",
    "cachedStopWords = set(stopwords.words(\"english\"))\n",
    "cachedStopWords.update((\"and\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"i'll\",\"don't\",\"would\",\"should\",\"shall\",\"hasn't\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let\",\"well\",\"just\",\"someone\",\"theres\",\"somebody\",\"didn't\",\"i've\",\"they're\",\"we're\",\"we'll\",\"we've\",\"they've\",\"they'd\",\"they'll\",\"again\",\"you're\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"many\",\"can't\",\"even\",\"cant\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"may\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\"))\n",
    "cachedTitles = [\"Mr.\",\"Mr\",\"Mrs.\",\"Mrs\",\"Miss\",\"Ms\",\"Sen.\"]\n",
    "prep_list=[\"in\",\"at\",\"of\",\"on\",\"and\",\"by\"] #includes common conjunction as well\n",
    "article_list=[\"a\",\"an\",\"the\"]\n",
    "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
    "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "chat_word_list=[\"thank\",\"thanks\",\"congrats\",\"whoa\",\"hey\",\"hi\",\"huh\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fuck\",\"wtf\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
    "#string.punctuation.extend('“','’','”')\n",
    "#---------------------Existing Lists--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rreplace(s, old, new, occurrence):\n",
    "    if s.endswith(old):\n",
    "        li = s.rsplit(old, occurrence)\n",
    "        return new.join(li)\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#candidate: 'frequency','length', 'capitalized', 'start_of_sentence', 'abbreviation', 'all_capitalized','is_csl','title','has_number','date_indicator','is_apostrophed','has_intermediate_punctuation','ends_like_verb','ends_like_adverb','change_in_capitalization','has_topic_indicator'\n",
    "def insert_dict(candidate,NE_container,candidateBase,tweetID,sentenceID):\n",
    "    key=(((candidate.phraseText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
    "    key= rreplace(rreplace(rreplace(key,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "    \n",
    "    if key in NE_container:\n",
    "        feature_list=NE_container[key]\n",
    "        feature_list[0]+=1\n",
    "        for index in [0,1,2,3,4,5,6,7,9,10,11,13]:\n",
    "            if (candidate.features[index]==True):\n",
    "                feature_list[index+2]+=1\n",
    "        for index in [8,12]:\n",
    "            if (candidate.features[index]!=-1):\n",
    "                feature_list[index+2]+=1\n",
    "    else:\n",
    "        feature_list=[0]*16\n",
    "        feature_list[0]+=1\n",
    "        feature_list[1]=candidate.length\n",
    "        #call background process to check for non capitalized occurences\n",
    "        for index in [0,1,2,3,4,5,6,7,9,10,11,13]:\n",
    "            if (candidate.features[index]==True):\n",
    "                feature_list[index+2]+=1\n",
    "        for index in [8,12]:\n",
    "            if (candidate.features[index]!=-1):\n",
    "                feature_list[index+2]+=1\n",
    "        NE_container[key] = feature_list\n",
    "    \n",
    "    #insert in candidateBase\n",
    "    if key in candidateBase.keys():\n",
    "        candidateBase[key]=candidateBase[key]+[str(tweetID)+\":\"+str(sentenceID)]\n",
    "    else:\n",
    "        candidateBase[key]=[str(tweetID)+\":\"+str(sentenceID)]\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printList(mylist):\n",
    "    print(\"[\"),\n",
    "    #print \"[\",\n",
    "    for item in mylist:\n",
    "        if item != None:\n",
    "            if isinstance(item,ne.NE_candidate):\n",
    "                item.print_obj()\n",
    "                #print (item.phraseText)\n",
    "            else:\n",
    "                print (item+\",\", end=\"\")\n",
    "                #print item+\",\",\n",
    "    #print \"]\"\n",
    "    print(\"]\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten(mylist, outlist,ignore_types=(str, bytes, ne.NE_candidate)):\n",
    "    \n",
    "    if mylist !=[]:\n",
    "        for item in mylist:\n",
    "            #print not isinstance(item, ne.NE_candidate)\n",
    "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
    "                flatten(item, outlist)\n",
    "            else:\n",
    "                if isinstance(item,ne.NE_candidate):\n",
    "                    item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
    "                    item.reset_length()\n",
    "                else:\n",
    "                    item=item.strip(' \\t\\n\\r')\n",
    "                outlist.append(item)\n",
    "    return outlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def consecutive_cap(tweetWordList_cappos,tweetWordList):\n",
    "    output=[]\n",
    "    #identifies consecutive numbers in the sequence\n",
    "    for k, g in groupby(enumerate(tweetWordList_cappos), lambda element: element[0]-element[1]):\n",
    "        output.append(list(map(itemgetter(1), g)))\n",
    "    count=0\n",
    "    if output:        \n",
    "        final_output=[output[0]]\n",
    "        for first, second in (zip(output,output[1:])):\n",
    "            if ((second[0]-first[-1])==2) & (tweetWordList[first[-1]+1].lower() in prep_list):\n",
    "                (final_output[-1]).extend([first[-1]+1]+second)\n",
    "            elif((second[0]-first[-1])==3) & (tweetWordList[first[-1]+1].lower() in prep_list)& (tweetWordList[first[-1]+2].lower() in article_list):\n",
    "                (final_output[-1]).extend([first[-1]+1]+[first[-1]+2]+second)\n",
    "            else:\n",
    "                final_output.append(second)\n",
    "                #merge_positions.append(False)\n",
    "    else:\n",
    "        final_output=[]\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#basically splitting the original NE_candidate text and building individual object from each text snippet\n",
    "def build_custom_NE(phrase,pos,prototype,feature_index,feature_value):\n",
    "    #print(\"Enters\")\n",
    "    position=pos\n",
    "    custom_NE= ne.NE_candidate(phrase,position)\n",
    "    for i in range(14):\n",
    "        custom_NE.set_feature(i,prototype.features[i])\n",
    "    custom_NE.set_feature(feature_index,feature_value)\n",
    "    if (feature_index== ne.is_csl) & (feature_value== True):\n",
    "        custom_NE.set_feature(ne.start_of_sentence, False)\n",
    "    custom_NE=entity_info_check(custom_NE)\n",
    "    return custom_NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abbrv_algo(ne_element):\n",
    "    '''abbreviation algorithm \n",
    "    trailing apostrophe:\n",
    "           |period:\n",
    "           |     multiple letter-period sequence:\n",
    "           |         all caps\n",
    "           | non period:\n",
    "           |     ?/! else drop apostrophe\n",
    "    else:\n",
    "        unchanged\n",
    "    '''\n",
    "    phrase= ne_element.phraseText\n",
    "    #print(\"=>\"+phrase)\n",
    "    #since no further split occurs we can set remaining features now\n",
    "    ne_element.set_feature(ne.capitalized, True)\n",
    "    if ne_element.phraseText.isupper():\n",
    "        ne_element.set_feature(ne.all_capitalized, True)\n",
    "    else:\n",
    "        ne_element.set_feature(ne.all_capitalized, False)\n",
    "        \n",
    "    abbreviation_flag=False\n",
    "    p=re.compile(r'[^a-zA-Z\\d\\s]$')\n",
    "    match_list = p.findall(phrase)\n",
    "    if len(match_list)>0:\n",
    "        #print(\"Here\")\n",
    "        if phrase.endswith('.'):\n",
    "            p1= re.compile(r'([a-zA-Z][\\.]\\s*)')\n",
    "            match_list = p1.findall(phrase)\n",
    "            if ((len(match_list)>1) & (len(phrase)<6)):\n",
    "                #print (\"1. Found abbreviation: \"+phrase)\n",
    "                abbreviation_flag= True\n",
    "            else:\n",
    "                phrase= phrase[:-1]\n",
    "        else:\n",
    "            phrase= phrase[:-1]\n",
    "            #if not (phrase.endswith('?')|phrase.endswith('!')|phrase.endswith(')')|phrase.endswith('>')):\n",
    "                #phrase= phrase[:-1]\n",
    "    else:\n",
    "        p2=re.compile(r'([^a-zA-Z0-9_\\s])')\n",
    "        match_list = p2.findall(phrase)\n",
    "        if ((len(match_list)==0) & (phrase.isupper()) & (len(phrase)<7)& (len(phrase)>1)):\n",
    "            #print (\"2. Found abbreviation!!: \"+phrase)\n",
    "            abbreviation_flag= True\n",
    "        else:\n",
    "            #print(\"Here-> \"+phrase)\n",
    "            p3= re.compile(r'([A-Z][.][A-Z])')\n",
    "            p4= re.compile(r'\\s')\n",
    "            match_list = p3.findall(phrase)\n",
    "            match_list1 = p4.findall(phrase)\n",
    "            if ((len(match_list)>0) & (len(match_list1)==0)):\n",
    "                abbreviation_flag= True\n",
    "                #print (\"3. Found abbreviation!!: \"+phrase)\n",
    "            \n",
    "    #element= ne.NE_candidate(phrase.strip())\n",
    "    ne_element.phraseText=phrase\n",
    "    ne_element.reset_length()\n",
    "    ne_element.set_feature(ne.abbreviation, abbreviation_flag)\n",
    "    return ne_element\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def punct_clause(NE_phrase_in):\n",
    "    \n",
    "    NE_phrases=entity_info_check(NE_phrase_in)\n",
    "    cap_phrases=NE_phrases.phraseText.strip()\n",
    "    #print (cap_phrases)\n",
    "    if (re.compile(r'[^a-zA-Z0-9_\\s]')).findall(cap_phrases):\n",
    "        #case of intermediate punctuations: handles abbreviations\n",
    "        p1= re.compile(r'(?:[a-zA-Z0-9][^a-zA-Z0-9_\\s]\\s*)')\n",
    "        match_lst = p1.findall(cap_phrases)\n",
    "        if match_lst:\n",
    "            index= (list( p1.finditer(cap_phrases) )[-1]).span()[1]\n",
    "        \n",
    "        p= re.compile(r'[^a-zA-Z\\d\\s]')\n",
    "        match_list = p.findall(cap_phrases)\n",
    "\n",
    "        p2=re.compile(r'[^a-zA-Z\\d\\s]$') #ends with punctuation\n",
    "\n",
    "        if len(match_list)-len(match_lst)>0:\n",
    "            if (p2.findall(cap_phrases)):\n",
    "                #only strips trailing punctuations, not intermediate ones following letters\n",
    "                cap_phrases = cap_phrases[0:index]+re.sub(p, '', cap_phrases[index:])\n",
    "                NE_phrases.phraseText= cap_phrases\n",
    "        \n",
    "    \n",
    "    #comma separated NEs\n",
    "    #lst=filter(lambda(word): word!=\"\", re.split('[,]', cap_phrases))\n",
    "    start_of_sentence_fix=NE_phrases.features[ne.start_of_sentence]\n",
    "    wordlst=list(filter(lambda word: word!=\"\", re.split('[,]', cap_phrases)))\n",
    "    if (NE_phrases.features[ne.date_indicator]==False) & (len(wordlst)>1):\n",
    "        pos=NE_phrases.position\n",
    "        combined=[]\n",
    "        prev=0\n",
    "        for i in range(len(wordlst)):\n",
    "            word=wordlst[i]\n",
    "            word_len=len(list(filter(lambda individual_word: individual_word!=\"\", re.split('[ ]', word))))\n",
    "            word_pos=pos[(prev):(prev+word_len)]\n",
    "            prev=prev+word_len\n",
    "            combined+=[[word]+word_pos]\n",
    "        \n",
    "        lst_nsw=list(filter(lambda element: (((str(element[0])).lower() not in cachedStopWords) & (len(str(element[0]))>1)) ,combined))\n",
    "        #print (lst_nsw)\n",
    "        final_lst= list(map(lambda element:build_custom_NE(str(element[0]),element[1:],NE_phrases,ne.is_csl,True), lst_nsw))\n",
    "        final_lst[0].set_feature(ne.start_of_sentence, NE_phrases.features[ne.start_of_sentence])\n",
    "    else:\n",
    "        NE_phrases.set_feature(ne.is_csl,False)\n",
    "        final_lst=[NE_phrases]\n",
    "    \n",
    "    #check abbreviation\n",
    "    final_lst= list(map(lambda phrase: abbrv_algo(phrase), final_lst))\n",
    "\n",
    "    \n",
    "    #print(lst)\n",
    "    return final_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%timeit -o\n",
    "def f(x,tweetWordList):\n",
    "\n",
    "    #list1=map(lambda word: check(tweetWordList[word], word), x)\n",
    "    list1=list(map(lambda word: tweetWordList[word]+\" \", x[:-1]))\n",
    "    phrase=\"\".join(list1)+tweetWordList[x[-1]]\n",
    "\n",
    "    if not ((phrase[0].isdigit()) & (len(x)==1)):\n",
    "        NE_phrase= ne.NE_candidate(phrase.strip(),x)\n",
    "        if 0 in x:\n",
    "            NE_phrase.set_feature(ne.start_of_sentence,True)\n",
    "        else:\n",
    "            NE_phrase.set_feature(ne.start_of_sentence,False)\n",
    "    else:\n",
    "        NE_phrase= ne.NE_candidate(\"JUST_DIGIT_ERROR\",[])\n",
    "\n",
    "    return NE_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def capCheck(word):\n",
    "    combined_list=[]+list(cachedStopWords)+prep_list+chat_word_list\n",
    "    if word.startswith('@'):\n",
    "        return False\n",
    "    elif \"<Hashtag\" in word:\n",
    "        return False\n",
    "    elif (((word.strip('“‘’”')).lstrip(string.punctuation)).rstrip(string.punctuation)).lower() in combined_list:\n",
    "        if(word!=\"The\"):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    elif word[0].isdigit():\n",
    "        return True\n",
    "    else:\n",
    "        p=re.compile(r'^[\\W]*[A-Z]')\n",
    "        l= p.match(word)\n",
    "        if l:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_check(ne_phrase):\n",
    "    title_flag=False\n",
    "    words=ne_phrase.phraseText.split()\n",
    "    for word in words:\n",
    "        if word in cachedTitles:\n",
    "            title_flag= True\n",
    "            break\n",
    "    ne_phrase.set_feature(ne.title,title_flag)\n",
    "    return ne_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entity_info_check(ne_phrase):\n",
    "    flag1=False #has number\n",
    "    flag3=False\n",
    "    flag_ind=[] #is number\n",
    "    month_ind=[]\n",
    "    date_num_holder=[]\n",
    "    words=ne_phrase.phraseText.split()\n",
    "    for word in words:\n",
    "        word=(word.strip()).rstrip(string.punctuation).lower()\n",
    "        punct_flag=False\n",
    "        for char in word:\n",
    "            if ((char in string.punctuation)|(char in ['“','’','”','…'])):\n",
    "                punct_flag=True\n",
    "                break\n",
    "        #if ((not word.isalpha())& (not \"'s\" in word) & (not \"’s\" in word)):\n",
    "        if ((not word.isalpha())& (not punct_flag)):\n",
    "            flag_ind+=[True]\n",
    "            if word.isdigit():\n",
    "                date_num_holder+=['num']\n",
    "            else:\n",
    "                date_num_holder+=['alpha']\n",
    "        else:\n",
    "            flag_ind+=[False]\n",
    "            if word in month_list:\n",
    "                month_ind+=[True]\n",
    "                date_num_holder+=['month']\n",
    "            elif word in day_list:\n",
    "                date_num_holder+=['day']\n",
    "            elif word in prep_list:\n",
    "                date_num_holder+=['preposition']\n",
    "            elif word in article_list:\n",
    "                date_num_holder+=['article']\n",
    "            else:\n",
    "                #print(\"=>\"+word)\n",
    "                date_num_holder+=['string']\n",
    "    if True in flag_ind:\n",
    "        flag1=True\n",
    "    if True in month_ind:\n",
    "        flag3=True\n",
    "    ne_phrase.set_feature(ne.has_number,flag1)\n",
    "    ne_phrase.set_feature(ne.date_indicator,flag3)\n",
    "    ne_phrase.set_date_num_holder(date_num_holder)\n",
    "    return ne_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing commonly used expletives, enunciated chat words and other common words (like days of the week, common expressions)\n",
    "def slang_remove(ne_phrase):\n",
    "    phrase=(ne_phrase.phraseText.strip()).rstrip(string.punctuation).lower()\n",
    "    p1= re.compile(r'([A-Za-z]+)\\1\\1{1,}')\n",
    "    match_lst = p1.findall(phrase)\n",
    "    if phrase in article_list:\n",
    "        return True\n",
    "    elif phrase in day_list:\n",
    "        return True\n",
    "    elif phrase in month_list:\n",
    "        return True\n",
    "    elif match_lst:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apostrope_check(ne_phrase):\n",
    "    apostrophe=\"'s\"\n",
    "    bad_apostrophe=\"’s\"\n",
    "    phrase=(ne_phrase.phraseText.strip()).rstrip(string.punctuation).lower()\n",
    "    if (apostrophe in phrase):\n",
    "        if (phrase.endswith(apostrophe)):\n",
    "            ne_phrase.set_feature(ne.is_apostrophed,0)\n",
    "        else:\n",
    "            #print(phrase.find(apostrophe))\n",
    "            ne_phrase.set_feature(ne.is_apostrophed,phrase.find(apostrophe))\n",
    "    elif (bad_apostrophe in phrase):\n",
    "        if phrase.endswith(bad_apostrophe):\n",
    "            ne_phrase.set_feature(ne.is_apostrophed,0)\n",
    "        else:\n",
    "            #print(phrase.find(apostrophe))\n",
    "            ne_phrase.set_feature(ne.is_apostrophed,phrase.find(bad_apostrophe))\n",
    "    else:\n",
    "        ne_phrase.set_feature(ne.is_apostrophed,-1)\n",
    "    return ne_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def punctuation_check(ne_phrase):\n",
    "    holder=[]\n",
    "    punctuation_holder=[]\n",
    "    flag_holder=[]\n",
    "    phrase=(ne_phrase.phraseText.strip()).rstrip(string.punctuation).lower()\n",
    "    for i in range(len(phrase)):\n",
    "        if (phrase[i] in string.punctuation):\n",
    "            holder+=[i]\n",
    "    for i in holder:\n",
    "        if ((i<(len(phrase)-1)) & (phrase[i]==\"'\") & (phrase[i+1]==\"s\")):\n",
    "            flag_holder+=[False]\n",
    "        elif ((i==(len(phrase)-1)) & (phrase[i]==\"'\")):\n",
    "            flag_holder+=[False]\n",
    "        else:\n",
    "            flag_holder+=[True]\n",
    "            punctuation_holder+=[i]\n",
    "    #print(flag_holder)\n",
    "    ne_phrase.set_punctuation_holder(punctuation_holder)\n",
    "    if True in flag_holder:\n",
    "        ne_phrase.set_feature(ne.has_intermediate_punctuation,True)\n",
    "    else:\n",
    "        ne_phrase.set_feature(ne.has_intermediate_punctuation,False)\n",
    "    return ne_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tense_check(ne_phrase):\n",
    "    words=(((ne_phrase.phraseText.strip()).rstrip(string.punctuation)).lower()).split()\n",
    "    verb_flag=False\n",
    "    adverb_flag=False\n",
    "    if (len(words)==1):\n",
    "        if words[0].endswith(\"ing\"):\n",
    "            verb_flag=True\n",
    "        if words[0].endswith(\"ly\"):\n",
    "            adverb_flag=True\n",
    "    ne_phrase.set_feature(ne.ends_like_verb,verb_flag)\n",
    "    ne_phrase.set_feature(ne.ends_like_adverb,adverb_flag)\n",
    "    return ne_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def capitalization_change(ne_element):\n",
    "    phrase=((ne_element.phraseText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()\n",
    "    val=-1\n",
    "    topic_indicator=False\n",
    "    p1= re.compile(r'[A-Z]*\\s*[A-Z]{4,}[^A-Za-z]*\\s+[A-Za-z]+') #BREAKING: Toronto Raptors\n",
    "    p2= re.compile(r'([A-Z]{1}[a-z]+)+[^A-Za-z]*\\s+[A-Z]{4,}') #The DREAMIEST LAND\n",
    "    match_lst1 = p1.findall(phrase)\n",
    "    match_lst2 = p2.findall(phrase)\n",
    "    if (match_lst1):\n",
    "        if not phrase.isupper():\n",
    "            p3=re.compile(r'[A-Z]*\\s*[A-Z]{4,}[^A-Za-z]*\\s+')\n",
    "            val=list(p3.finditer(phrase))[-1].span()[1]\n",
    "            if(\":\" in phrase):\n",
    "                topic_indicator=True\n",
    "            ne_element.set_feature(ne.change_in_capitalization,val)\n",
    "    elif (match_lst2):\n",
    "        #print (\"GOTIT2: \"+phrase)\n",
    "        p3=re.compile(r'([A-Z]{1}[a-z]+)+')\n",
    "        val=list(p3.finditer(phrase))[-1].span()[1]\n",
    "        ne_element.set_feature(ne.change_in_capitalization,val)\n",
    "    else:\n",
    "        ne_element.set_feature(ne.change_in_capitalization,val)\n",
    "    ne_element.set_feature(ne.has_topic_indicator,topic_indicator)\n",
    "    return ne_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trueEntity_process(tweetWordList_cappos,tweetWordList,q):\n",
    "    \n",
    "    #returns list with position of consecutively capitalized words\n",
    "    output_unfiltered = consecutive_cap(tweetWordList_cappos,tweetWordList)\n",
    "    output_1= list(filter(lambda list_in: len(list_in)<7, output_unfiltered))\n",
    "    output= list(filter(lambda list_in: list_in!=[0], output_1))\n",
    "\n",
    "    #consecutive capitalized phrases \n",
    "    consecutive_cap_phrases1=list(map(lambda x: f(x,tweetWordList), output))\n",
    "\n",
    "    consecutive_cap_phrases=list(filter(lambda candidate:(candidate.phraseText!=\"JUST_DIGIT_ERROR\"),consecutive_cap_phrases1))\n",
    "\n",
    "    \n",
    "    #implement the punctuation clause\n",
    "    ne_List_pc=flatten(list(map(lambda NE_phrase: punct_clause(NE_phrase), consecutive_cap_phrases)),[])\n",
    "    \n",
    "\n",
    "    #implement title detection\n",
    "    ne_List_titleCheck= list(map(lambda element: title_check(element), ne_List_pc))\n",
    "    \n",
    "    #implement slang check and remove\n",
    "    ne_List_slangCheck= list(filter(lambda element: not slang_remove(element), ne_List_titleCheck))\n",
    "    \n",
    "    #implement apostrophe, tense and punctuation marker with final number check\n",
    "    ne_List_apostropeCheck= list(map(lambda element: apostrope_check(element), ne_List_slangCheck))\n",
    "    ne_List_punctuationCheck= list(map(lambda element: punctuation_check(element), ne_List_apostropeCheck))\n",
    "    ne_List_numCheck=list(filter(lambda candidate: not (candidate.phraseText.lstrip(string.punctuation).rstrip(string.punctuation).strip()).isdigit(), ne_List_punctuationCheck))\n",
    "    ne_List_tenseCheck= list(map(lambda element: tense_check(element), ne_List_numCheck))\n",
    "    \n",
    "    #tracking sudden change in capitalization pattern\n",
    "    ne_List_allCheck= list(map(lambda element: capitalization_change(element), ne_List_tenseCheck))\n",
    "    q.put(ne_List_allCheck)\n",
    "    \n",
    "    #return ne_List_allCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "Total number of tokens processed: 45610\n",
      "Total number of candidate NEs extracted: 2503\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''This is the main module. I am not explicitly writing it as a function as I am not sure what argument you are \n",
    "passing.However you can call this whole cell as a function and it will call the rest of the functions in my module\n",
    "to extract candidates and features\n",
    "'''\n",
    "\n",
    "'''#reads input from the database file and converts to a dataframe. You can change this part accordingly and\n",
    "#directly convert argument tuple to the dataframe'''\n",
    "\n",
    "#Collection.csv 500Sample.csv 3.2KSample.csv\n",
    "df = read_csv('/home/satadisha/Desktop/GitProjects/ELTweetTracker/3.2KSample.csv', index_col='ID', header=0, encoding='utf-8')\n",
    "\n",
    "#output.csv\n",
    "df_out= DataFrame(columns=('tweetID', 'sentID', 'hashtags', 'user', 'usertype', 'TweetSentence', 'phase1Candidates'))\n",
    "\n",
    "#%%timeit -o\n",
    "#module_capital_punct.main:\n",
    "'''I am running this for 100 iterations for testing purposes. Of course you no longer need this for loop as you are\n",
    "#running one tuple at a time'''\n",
    "NE_container={}\n",
    "candidateBase={}\n",
    "#NE_container=DataFrame(columns=('candidate', 'frequency', 'capitalized', 'start_of_sentence', 'abbreviation', 'all_capitalized','is_csl','title','has_number','date_indicator','is_apostrophed','has_intermediate_punctuation','ends_like_verb','ends_like_adverb','change_in_capitalization','has_topic_indicator'))\n",
    "\n",
    "count=0\n",
    "ne_count=0\n",
    "userMention_count=0\n",
    "token_count=0\n",
    "\n",
    "NE_list_phase1=[]\n",
    "UserMention_list=[]\n",
    "ME_EXTR=Mention.Mention_Extraction()\n",
    "\n",
    "#--------------------------------------PHASE I---------------------------------------------------\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    #hashtags=str(row['Discussion'])\n",
    "    hashtags=str(row['HashTags'])\n",
    "    user=str(row['User'])\n",
    "    userType=str(row['User Type'])\n",
    "    tweetText=str(row['TweetText'])\n",
    "\n",
    "    #print(str(index)+\". \"+userType+\":=>\\n\"+tweetText)\n",
    "\n",
    "    #capitalization module\n",
    "    #if all words are capitalized:\n",
    "    if tweetText.isupper():\n",
    "        print(\"\",end=\"\")\n",
    "        #print (\"All caps module\\n\")\n",
    "        outrow=[str(index), str(sen_index), hashtags, user, userType, sentence, \"\"]\n",
    "        df_out.loc[len(df_out)]=outrow\n",
    "    elif tweetText.islower():\n",
    "        print(\"\",end=\"\")\n",
    "        outrow=[str(index), str(sen_index), hashtags, user, userType, sentence, \"\"]\n",
    "        df_out.loc[len(df_out)]=outrow\n",
    "        #print (\"All lower module\\n\")\n",
    "    else:\n",
    "        ne_List_final=[]\n",
    "        userMention_List_final=[]\n",
    "        #pre-modification: returns word list split at whitespaces; retains punctuation\n",
    "        tweetSentences=list(filter (lambda sentence: len(sentence)>1, tweetText.split('\\n')))\n",
    "        tweetSentenceList_inter=flatten(list(map(lambda sentText: sent_tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "        tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
    "\n",
    "        for sen_index in range(len(tweetSentenceList)):\n",
    "            sentence=tweetSentenceList[sen_index]\n",
    "            #print(sentence)\n",
    "            tweetWordList= sentence.split()\n",
    "\n",
    "            token_count+=len(tweetWordList)\n",
    "            #print (tweetWordList)\n",
    "            #returns position of words that are capitalized\n",
    "            tweetWordList_cappos = list(map(lambda element : element[0], filter(lambda element : capCheck(element[1]), enumerate(tweetWordList))))\n",
    "            \n",
    "            #returns list of @userMentions\n",
    "            userMentionswPunct=list(filter(lambda phrase: phrase.startswith('@'), tweetWordList))\n",
    "            userMentions=list(map(lambda mention: mention.rstrip(string.punctuation), userMentionswPunct))\n",
    "\n",
    "            #non @usermentions are processed in this function to find non @, non hashtag Entities---- thread 1\n",
    "            q = queue.Queue()\n",
    "            threading.Thread(target=trueEntity_process, args=(tweetWordList_cappos,tweetWordList,q)).start()\n",
    "                #ne_List_allCheck= trueEntity_process(tweetWordList_cappos,tweetWordList)\n",
    "\n",
    "            userMention_count+=len(userMentions)\n",
    "            userMention_List_final+=userMentions                \n",
    "            #function to process and store @ user mentions---- thread 2\n",
    "            threading.Thread(target=ME_EXTR.ComputeAll, args=(userMention_List_final,)).start()\n",
    "\n",
    "            ne_List_allCheck= q.get()\n",
    "            ne_count+=len(ne_List_allCheck)\n",
    "            ne_List_final+=ne_List_allCheck\n",
    "\n",
    "            #write row to output dataframe\n",
    "            phase1Out=\"\"\n",
    "            for candidate in ne_List_allCheck:\n",
    "                phase1Out+=candidate.phraseText.strip()+\"|| \" \n",
    "            outrow=[str(index), str(sen_index), hashtags, user, userType, sentence, phase1Out]\n",
    "            df_out.loc[len(df_out)]=outrow\n",
    "\n",
    "        for candidate in ne_List_final:\n",
    "            insert_dict (candidate,NE_container,candidateBase,index,sen_index)\n",
    "        #printList(ne_List_final)\n",
    "        #if(userMention_List_final):\n",
    "        #    print(userMention_List_final)\n",
    "\n",
    "        NE_list_phase1+=ne_List_final\n",
    "        UserMention_list+=userMention_List_final\n",
    "        #print (\"\\n\")\n",
    "\n",
    "#unmerged candidate table and candidateBase\n",
    "sorted_NE_container =OrderedDict(sorted(NE_container.items(), key=lambda t: t[1],reverse=True))\n",
    "sorted_candidateBase =OrderedDict(sorted(candidateBase.items(), key=lambda t: len(t[1]),reverse=True))\n",
    "\n",
    "#computing z-score of frequency\n",
    "frequency_array = np.array(list(map(lambda val: val[0], sorted_NE_container.values())))\n",
    "zscore_array=stats.zscore(frequency_array)\n",
    "\n",
    "index=0\n",
    "#for key, val in sorted_NE_container:\n",
    "fieldnames=['candidate','freq','length','cap','start_of_sen','abbrv','all_cap','is_csl','title','has_no','date','is_apostrp','has_inter_punct','ends_verb','ends_adverb','change_in_cap','topic_ind','@mention','z_score']\n",
    "\n",
    "for key in sorted_NE_container.keys():\n",
    "    #alias=computeAlias(key)\n",
    "    val=sorted_NE_container[key]+[str(ME_EXTR.checkInDictionary(key))]+[str(zscore_array[index])]\n",
    "    index+=1\n",
    "    sorted_NE_container[key]=val\n",
    "    #print (key+\" : \"+str(sorted_NE_container[key]))\n",
    "\n",
    "#with open('candidates500.csv', 'w') as output_candidate:\n",
    "with open('candidates.csv', 'w') as output_candidate:\n",
    "    writer = csv.writer(output_candidate)\n",
    "    writer.writerow(fieldnames)\n",
    "    for k, v in sorted_NE_container.items():\n",
    "        writer.writerow([k] + v)\n",
    "\n",
    "#ME_EXTR.PrintDictionary()\n",
    "print(\"**********************************************************\")\n",
    "\n",
    "print(\"Total number of tokens processed: \"+str(token_count))\n",
    "print (\"Total number of candidate NEs extracted: \"+str(len(sorted_NE_container.keys())))\n",
    "\n",
    "df_out.to_csv('TweetBase.csv')\n",
    "#df_out.to_csv('TweetBase500.csv')\n",
    "#--------------------------------------PHASE I---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate neighbours with Jaccard similarity ['m2']\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------PHASE II---------------------------------------------------\n",
    "'''set1 = set(['Melania','Trump'])\n",
    "set2 = set(['Donald','Trump'])\n",
    "set3 = set(['Jared','Kushner'])\n",
    "\n",
    "m1 = MinHash(num_perm=200)\n",
    "m2 = MinHash(num_perm=200)\n",
    "m3 = MinHash(num_perm=200)\n",
    "for d in set1:\n",
    "    m1.update(d.encode('utf8'))\n",
    "\n",
    "for d in set2:\n",
    "    m2.update(d.encode('utf8'))\n",
    "for d in set3:\n",
    "    m3.update(d.encode('utf8'))\n",
    "\n",
    "# Create LSH index\n",
    "lsh = MinHashLSH(threshold=0.0, num_perm=200)\n",
    "lsh.insert(\"m2\", m2)\n",
    "lsh.insert(\"m3\", m3)\n",
    "result = lsh.query(m1)\n",
    "print(\"Approximate neighbours with Jaccard similarity\", result)\n",
    "\n",
    "\n",
    "candidates=[\"donald trump\",\"melania trump\", \"obama\",\"barack obama\",\"barack\"]\n",
    "listofMinhash=[]\n",
    "m=MinHash(num_perm=200)\n",
    "candidate0=set(candidates[0].split())\n",
    "for d in candidate0:\n",
    "    m.update(d.encode('utf8'))\n",
    "listofMinhash.append(m)\n",
    "lsh = MinHashLSH(threshold=0.0, num_perm=200)\n",
    "lsh.insert(\"m2\", m2)\n",
    "for candidate in candidates[1:]:'''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling articles...\n",
      "\n",
      "Shingling 2503 candidates took 0.25 sec.\n",
      "\n",
      "Average shingles per doc: 2.09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"Shingling articles...\")\n",
    "\n",
    "# The current shingle ID value to assign to the next new shingle we \n",
    "# encounter. When a shingle gets added to the dictionary, we'll increment this\n",
    "# value.\n",
    "curShingleID = 0\n",
    "\n",
    "# Create a dictionary of the articles, mapping the article identifier (e.g., \n",
    "# \"t8470\") to the list of shingle IDs that appear in the document.\n",
    "candidatesAsShingleSets = {};\n",
    "  \n",
    "candidateNames = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "totalShingles = 0\n",
    "\n",
    "for k in range(0, len(sorted_NE_container.keys())):\n",
    "    # Read all of the words (they are all on one line) and split them by white space.\n",
    "    words = list(sorted_NE_container.keys())[k].split(\" \")\n",
    "    \n",
    "    # Retrieve the article ID, which is the first word on the line.  \n",
    "    candidateID = k\n",
    "    \n",
    "    # Maintain a list of all document IDs.  \n",
    "    candidateNames.append(candidateID)\n",
    "    \n",
    "    \n",
    "    # 'shinglesInDoc' will hold all of the unique shingle IDs present in the current document.\n",
    "    #If a shingle ID occurs multiple times in the document,\n",
    "    # it will only appear once in the set (this is a property of Python sets).\n",
    "    shinglesInCandidate = set()\n",
    "    \n",
    "    # For each word in the document...\n",
    "    for index in range(0, len(words)):\n",
    "        \n",
    "        # Construct the shingle text by combining three words together.\n",
    "        shingle = words[index]\n",
    "        # Hash the shingle to a 32-bit integer.\n",
    "        #crc = binascii.crc32(\"\")\n",
    "        crc = binascii.crc32(bytes(shingle, encoding=\"UTF-8\")) & (0xffffffff)\n",
    "\n",
    "        # Add the hash value to the list of shingles for the current document. \n",
    "        # Note that set objects will only add the value to the set if the set \n",
    "        # doesn't already contain it. \n",
    "        shinglesInCandidate.add(crc)\n",
    "    \n",
    "    # Store the completed list of shingles for this document in the dictionary.\n",
    "    '''print(str(words)+\": \")\n",
    "    for i in shinglesInCandidate:\n",
    "        print('0x%08x' %i)'''\n",
    "    candidatesAsShingleSets[candidateID] = shinglesInCandidate\n",
    "    \n",
    "    # Count the number of shingles across all documents.\n",
    "    totalShingles = totalShingles + (len(words))\n",
    "\n",
    "\n",
    "# Report how long shingling took.\n",
    "print ('\\nShingling ' + str(str(len(sorted_NE_container.keys()))) + ' candidates took %.2f sec.' % (time.time() - t0))\n",
    " \n",
    "print ('\\nAverage shingles per doc: %.2f' % (totalShingles / len(sorted_NE_container.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random hash functions...\n",
      "\n",
      "Generating MinHash signatures for all candidates...\n",
      "\n",
      "Generating MinHash signatures took 0.05sec\n",
      "\n",
      "Comparing all signatures...\n",
      "trump: =>donald trump, =>president trump, =>melania trump, =>ivanka trump, =>trump tower, =>trump administration, =>the trump, =>trump university, =>team trump, =>pres trump, =>new: trump, =>donald j. trump, =>trump continues, =>mr. trump, =>trump admin, =>mr trump, =>4 trump, =>trump wh, =>trump transition, =>trump russia, =>trump associates, =>ft: donald trump, =>trump praises 'fox, =>lots of trump, =>trump locks horns, =>3% of trump, =>shade at trump, =>the dummy trump, =>trump dead sea, =>the hill trump, =>indict trump, =>trump music, =>trump moment, =>trump son-in-law, =>trump &;family, =>‘president trump, =>melanoma trump, =>icymi: trump, =>official: trump, =>spicer: trump, =>trace: trump, =>remember: trump, =>go trump, =>trump always, =>trump acts, =>trump walks, =>hope trump, =>trump says, =>rubbish trump, =>neither trump, =>trump wants, =>vote trump, =>trump 2017, =>unmasked' trump, =>scare trump, =>‘unmasked’ trump, =>trump 17, =>trump 2, =>2nd trump, =>trump 33, =>trump 28, =>trump 50-60, =>trump potus, =>love trump, =>trump approves, =>voting trump, =>trump impeachment, =>eric trump, =>behind trump, =>trump salary, =>trump could, =>trump election, =>trump unmasking, =>citizen trump, =>barron trump, =>admitted trump, =>comrade trump, =>nobody trump, =>obstruct trump, =>trump era, =>trump cuts, =>trump far, =>trump victories, =>destroy trump, =>fmr trump, =>tiffany trump, =>trump lies, =>toon trump, \n",
      "\n",
      "Comparing MinHash signatures took 0.02sec\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#                 Generate MinHash Signatures\n",
    "# =============================================================================\n",
    "numHashes=20\n",
    "numCandidates=len(sorted_NE_container.keys())\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "print ('Generating random hash functions...')\n",
    "\n",
    "# Record the maximum shingle ID that we assigned.\n",
    "maxShingleID = 2**32-1\n",
    "nextPrime = 4294967311\n",
    "\n",
    "\n",
    "# Our random hash function will take the form of:\n",
    "#   h(x) = (a*x + b) % c\n",
    "# Where 'x' is the input value, 'a' and 'b' are random coefficients, and 'c' is\n",
    "# a prime number just greater than maxShingleID.\n",
    "\n",
    "# Generate a list of 'k' random coefficients for the random hash functions,\n",
    "# while ensuring that the same value does not appear multiple times in the \n",
    "# list.\n",
    "def pickRandomCoeffs(k):\n",
    "    # Create a list of 'k' random values.\n",
    "    randList = []\n",
    "    \n",
    "    while k > 0:\n",
    "        # Get a random shingle ID.\n",
    "        randIndex = random.randint(0, maxShingleID) \n",
    "        # Ensure that each random number is unique.\n",
    "        while randIndex in randList:\n",
    "            randIndex = random.randint(0, maxShingleID) \n",
    "        # Add the random number to the list.\n",
    "        randList.append(randIndex)\n",
    "        k = k - 1\n",
    "    return randList\n",
    "\n",
    "# For each of the 'numHashes' hash functions, generate a different coefficient 'a' and 'b'.   \n",
    "coeffA = pickRandomCoeffs(numHashes)\n",
    "coeffB = pickRandomCoeffs(numHashes)\n",
    "\n",
    "print ('\\nGenerating MinHash signatures for all candidates...')\n",
    "\n",
    "# List of documents represented as signature vectors\n",
    "signatures = []\n",
    "\n",
    "# Rather than generating a random permutation of all possible shingles, \n",
    "# we'll just hash the IDs of the shingles that are *actually in the document*,\n",
    "# then take the lowest resulting hash code value. This corresponds to the index \n",
    "# of the first shingle that you would have encountered in the random order.\n",
    "\n",
    "# For each document...\n",
    "for candidateID in candidateNames:\n",
    "    \n",
    "    # Get the shingle set for this document.\n",
    "    shingleIDSet = candidatesAsShingleSets[candidateID]\n",
    "  \n",
    "    # The resulting minhash signature for this document. \n",
    "    signature = []\n",
    "  \n",
    "    # For each of the random hash functions...\n",
    "    for i in range(0, numHashes):\n",
    "        \n",
    "\n",
    "        # For each of the shingles actually in the document, calculate its hash code\n",
    "        # using hash function 'i'. \n",
    "\n",
    "        # Track the lowest hash ID seen. Initialize 'minHashCode' to be greater than\n",
    "        # the maximum possible value output by the hash.\n",
    "        minHashCode = nextPrime + 1\n",
    "\n",
    "        # For each shingle in the document...\n",
    "        for shingleID in shingleIDSet:\n",
    "            # Evaluate the hash function.\n",
    "            hashCode = (coeffA[i] * shingleID + coeffB[i]) % nextPrime \n",
    "\n",
    "            # Track the lowest hash code seen.\n",
    "            if hashCode < minHashCode:\n",
    "                minHashCode = hashCode\n",
    "\n",
    "        # Add the smallest hash code value as component number 'i' of the signature.\n",
    "        signature.append(minHashCode)\n",
    "\n",
    "    # Store the MinHash signature for this document.\n",
    "    signatures.append(signature)\n",
    "\n",
    "    # Calculate the elapsed time (in seconds)\n",
    "    elapsed = (time.time() - t0)\n",
    "\n",
    "print (\"\\nGenerating MinHash signatures took %.2fsec\" % elapsed)\n",
    "\n",
    "\n",
    "print ('\\nComparing all signatures...')\n",
    "  \n",
    "# Creates a N x N matrix initialized to 0.\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# For each of the test documents...\n",
    "for i in range(0, 1):\n",
    "#for i in range(0, numCandidates):\n",
    "    print(list(sorted_NE_container.keys())[i]+\": \",end=\"\")\n",
    "    # Get the MinHash signature for document i.\n",
    "    signature1 = signatures[i]\n",
    "    \n",
    "    # For each of the other test documents...\n",
    "    for j in range(i + 1, numCandidates):\n",
    "           \n",
    "        # Get the MinHash signature for document j.\n",
    "        signature2 = signatures[j]\n",
    "    \n",
    "        count = 0\n",
    "        # Count the number of positions in the minhash signature which are equal.\n",
    "        for k in range(0, numHashes):\n",
    "            count = count + (signature1[k] == signature2[k])\n",
    "    \n",
    "        # Record the percentage of positions which matched.    \n",
    "        estJSim= (count / numHashes)\n",
    "        #print(estJSim)\n",
    "        if (estJSim> 0.5):\n",
    "            print(\"=>\"+list(sorted_NE_container.keys())[j]+\", \",end=\"\") \n",
    "    print()\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "        \n",
    "print (\"\\nComparing MinHash signatures took %.2fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
